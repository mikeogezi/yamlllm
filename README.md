# YamlLLM ğŸš€

**A declarative DSL for transformer models with multi-backend code generation**

YamlLLM demonstrates key programming language principles through a domain-specific language (DSL) for defining transformer architectures. The system uses an intermediate representation (IR) to decouple the high-level YAML specification from backend-specific code generation.

```yaml
# Declarative model specification
name: TinyModel
embedding:
  vocab_size: 1000
  max_position_embeddings: 128
  embedding_dim: 256
  padding_idx: 0
  dropout: 0.0

num_layers: 2

layer:
  hidden_dim: 256
  attention:
    num_heads: 4
    dropout: 0.0
    bias: false
  ffn:
    intermediate_size: 512
    activation: relu
    dropout: 0.0
    bias: false
  layer_norm:
    eps: 1e-6
  residual_dropout: 0.0

tie_word_embeddings: false
```

**Key PL Concepts Demonstrated**:
- ğŸ“ **Domain-Specific Language**: High-level declarative syntax for transformers
- ğŸ”„ **Multi-Backend Compilation**: Single source â†’ PyTorch + JAX/Flax
- ğŸ—ï¸ **Intermediate Representation**: Decouples frontend from backend
- âœ… **Type Safety**: Schema validation with semantic checks
- ğŸ¯ **Code Generation**: Template-based + IR traversal

> **Academic Details**: See [`docs/REPORT.md`](docs/REPORT.md) for full PL analysis, IR design, and implementation.

## ğŸ—ï¸ Architecture

YamlLLM uses a three-layer architecture inspired by traditional compiler design:

**Pipeline**: YAML Schema â†’ Validated Config â†’ **IR** â†’ Backend Renderer â†’ Generated Code

### In more detail

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”
â”‚   YAML   â”‚â”€â”€â”€â”€â–¶â”‚ Parser â”‚â”€â”€â”€â”€â–¶â”‚  Schema  â”‚â”€â”€â”€â”€â–¶â”‚ IR Builder â”‚â”€â”€â”€â”€â–¶â”‚  IR  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”˜
                                                                        â”‚
                                                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                                        â–¼                              â–¼
                                                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                                  â”‚ PyTorch  â”‚                  â”‚   JAX    â”‚
                                                  â”‚ Renderer â”‚                  â”‚ Renderer â”‚
                                                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

The IR layer provides:
- **Abstraction**: Backend-agnostic model representation
- **Extensibility**: Easy to add new backends (e.g., ONNX, TensorFlow)
- **Optimization**: IR transformations before code generation

## ğŸš€ Quick Start

### 1. Define Your Model

Create a YAML config ([example](examples/tiny.yaml)):

```yaml
name: TinyModel
embedding:
  vocab_size: 1000
  max_position_embeddings: 128
  embedding_dim: 256
  positional_encoding: {type: learned}
layer:
  hidden_dim: 256
  attention: {num_heads: 4}
  ffn: {intermediate_size: 512, activation: relu}
  layer_norm: {type: layernorm, eps: 1e-6}
num_layers: 2
tie_word_embeddings: false
```

### 2. Generate Code

**PyTorch** ([example output](outputs/tiny_pytorch.py)):
```bash
python -m yamlllm.cli examples/tiny.yaml -o model.py
```

**JAX/Flax** ([example output](outputs/tiny_jax.py)):
```bash
python -m yamlllm.cli examples/tiny.yaml -b jax -o model_jax.py
```

**Both generate from the same YAML** - demonstrating multi-backend compilation!

### 3. Use Generated Models

**PyTorch**:
```python
from model import TinyModel
import torch

model = TinyModel()
input_ids = torch.randint(0, 1000, (1, 128))
logits = model(input_ids)  # (1, 128, 1000)
```

**JAX/Flax**:
```python
import jax
import jax.numpy as jnp
from model_jax import TinyModel

model = TinyModel()
params = model.init(jax.random.PRNGKey(0), jnp.ones((1, 128), dtype=jnp.int32))
logits = model.apply(params, jnp.ones((1, 128), dtype=jnp.int32))  # (1, 128, 1000)
```

4. **Serialized model architecture**
```python
TinyModel(
  (token_embedding): Embedding(1000, 256, padding_idx=0)
  (position_embedding): Embedding(128, 256)
  (embedding_dropout): Dropout(p=0.0, inplace=False)
  (layers): ModuleList(
    (0-1): 2 x DecoderLayer(
      (attention): CausalSelfAttention(
        (q_proj): Linear(in_features=256, out_features=256, bias=False)
        (k_proj): Linear(in_features=256, out_features=256, bias=False)
        (v_proj): Linear(in_features=256, out_features=256, bias=False)
        (out_proj): Linear(in_features=256, out_features=256, bias=False)
        (dropout): Dropout(p=0.0, inplace=False)
      )
      (ffn): FeedForward(
        (fc1): Linear(in_features=256, out_features=512, bias=False)
        (fc2): Linear(in_features=512, out_features=256, bias=False)
        (dropout): Dropout(p=0.0, inplace=False)
      )
      (ln1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
      (ln2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
      (residual_dropout): Dropout(p=0.0, inplace=False)
    )
  )
  (lm_head): Linear(in_features=256, out_features=1000, bias=False)
))
```

5. **Model Diagram**

![Model Diagram](assets/diagram.png)

## ğŸ› ï¸ CLI Usage

```bash
python -m yamlllm.cli [-h] [-o OUTPUT] [--backend {pytorch,jax}] input

positional arguments:
  input                 Path to YAML configuration file

options:
  -h, --help            show this help message and exit
  -o OUTPUT, --output OUTPUT
                        Output file path (default: print to stdout)
  --backend {pytorch,jax}
                        Target backend (default: pytorch)
```

## ğŸ“š Supported Architectures

YamlLLM supports defining architectures similar to:

| Architecture | YAML Example |
|--------------|--------------|
| **LLaMA** | `examples/llama-style.yaml` |
| **GPT-2** | `examples/gpt-mini.yaml` |
| **Mistral** | `examples/gqa-model.yaml` |
| **DeepSeek** | `examples/mla-model.yaml` |
| **Bloom** | `examples/alibi-model.yaml` |

See [docs/FEATURES.md](docs/FEATURES.md) for full configuration options.

## ğŸ” IR Visualization

Visualize the intermediate representation structure (see a Mermaid diagram [example](outputs/diagram.mmd)):

```bash
# ASCII tree view
python scripts/visualize_ir.py examples/tiny.yaml

# Mermaid diagram
python scripts/visualize_ir.py examples/tiny.yaml --format mermaid -o diagram.mmd
```

Example output for ASCII tree view:
```
Model: TinyModel
â”œâ”€ Vocab Size: 1000
â”œâ”€ Hidden Dim: 256
â””â”€ Num Layers: 2

Embedding Modules:
â”œâ”€â”€ token_embedding (module) [num_embeddings=1000, embedding_dim=256, padding_idx=0]
â””â”€â”€ position_embedding (module) [num_embeddings=128, embedding_dim=256, padding_idx=None]

Decoder Layers (2):
â”œâ”€â”€ Layer 0
â”‚   â”œâ”€â”€ attention (module) [num_heads=4, head_dim=64, hidden_dim=256, ...]
â”‚   â”‚   â”œâ”€â”€ q_proj (module) [in_features=256, out_features=256, bias=False]
â”‚   â”‚   â”œâ”€â”€ k_proj (module) [in_features=256, out_features=256, bias=False]
â”‚   â”‚   â”œâ”€â”€ v_proj (module) [in_features=256, out_features=256, bias=False]
â”‚   â”‚   â””â”€â”€ out_proj (module) [in_features=256, out_features=256, bias=False]
â”‚   â”œâ”€â”€ ln1 (module) [normalized_shape=256, eps=1e-6, elementwise_affine=True]
â”‚   â”œâ”€â”€ ffn (module) [hidden_dim=256, intermediate_size=512, activation=relu, ...]
â”‚   â”‚   â”œâ”€â”€ fc1 (module) [in_features=256, out_features=512, bias=False]
â”‚   â”‚   â””â”€â”€ fc2 (module) [in_features=512, out_features=256, bias=False]
â”‚   â””â”€â”€ ln2 (module) [normalized_shape=256, eps=1e-6, elementwise_affine=True]
â””â”€â”€ Layer 1
    â”œâ”€â”€ attention (module) [num_heads=4, head_dim=64, hidden_dim=256, ...]
    â”‚   â”œâ”€â”€ q_proj (module) [in_features=256, out_features=256, bias=False]
    â”‚   â”œâ”€â”€ k_proj (module) [in_features=256, out_features=256, bias=False]
    â”‚   â”œâ”€â”€ v_proj (module) [in_features=256, out_features=256, bias=False]
    â”‚   â””â”€â”€ out_proj (module) [in_features=256, out_features=256, bias=False]
    â”œâ”€â”€ ln1 (module) [normalized_shape=256, eps=1e-6, elementwise_affine=True]
    â”œâ”€â”€ ffn (module) [hidden_dim=256, intermediate_size=512, activation=relu, ...]
    â”‚   â”œâ”€â”€ fc1 (module) [in_features=256, out_features=512, bias=False]
    â”‚   â””â”€â”€ fc2 (module) [in_features=512, out_features=256, bias=False]
    â””â”€â”€ ln2 (module) [normalized_shape=256, eps=1e-6, elementwise_affine=True]

Output Modules:
â””â”€â”€ lm_head (module) [in_features=256, out_features=1000, bias=False]
```

## ğŸ­ Train a Character-Level Shakespeare Model

See your model learn to write like Shakespeare! ([full training log](outputs/train_pytorch.log))

```bash
python scripts/train.py --config examples/tiny.yaml --steps 1000
```

**Sample output after training (for 3000 steps)**:
```
=== Final Generation ===
To be, or not to be, that is the question:
Whether 'tis nobler in the mind to suffer
The slings and arrows of outrageous fortune,
Or to take arms against a sea of troubles
And by opposing end them. To dieâ€”to sleep,
No m
```

The model learns character-level patterns from Shakespeare and generates new text! ğŸ­

## ğŸ§ª Testing

Run the test suite to ensure everything is working:

```bash
python -m pytest tests/
```

Project carried out with â¤ï¸ and the help of Cursor and Antigravity.