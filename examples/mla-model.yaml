name: MLAModel
embedding:
  vocab_size: 50257
  max_position_embeddings: 2048
  embedding_dim: 768
  padding_idx: null
  dropout: 0.1

num_layers: 12

layer:
  hidden_dim: 768
  attention:
    num_heads: 12
    dropout: 0.1
    bias: true
    mechanism: mla
    mla_latent_dim: 192  # 768 / 4, reduces KV cache size
    mla_rank: 8
  ffn:
    intermediate_size: 3072
    activation: gelu
    dropout: 0.1
    bias: true
  layer_norm:
    type: layernorm
    eps: 1e-5
  residual_dropout: 0.1
  norm_placement: pre

tie_word_embeddings: true

