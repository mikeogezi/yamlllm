name: PostNormModel
embedding:
  vocab_size: 50257
  max_position_embeddings: 1024
  embedding_dim: 768
  padding_idx: null
  dropout: 0.1

num_layers: 12

layer:
  hidden_dim: 768
  attention:
    num_heads: 12
    dropout: 0.1
    bias: true
    mechanism: standard
  ffn:
    intermediate_size: 3072
    activation: geglu  # GeGLU gated activation
    use_gated_activation: true
    dropout: 0.1
    bias: true
  layer_norm:
    type: layernorm
    eps: 1e-5
  residual_dropout: 0.1
  norm_placement: post  # post-norm architecture (like original Transformer)

initialization:
  type: xavier_uniform
  gain: 1.0

tie_word_embeddings: true

